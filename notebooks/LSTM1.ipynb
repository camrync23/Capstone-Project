{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a506e31a-1751-4989-9e53-011610273021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72b416c7-795f-4bfe-83bd-ba501f17c970",
   "metadata": {},
   "outputs": [],
   "source": [
    "### **LOAD & PREPROCESS DATA (2M Rows for Speed)**\n",
    "df = pd.read_parquet('../data/cleaned_data_snappy.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "013f6909-e51d-4f19-b740-9755e62ebf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 2 million rows for faster training while maintaining statistical distribution\n",
    "df_sample = df.sample(n=2000000, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f95ad72e-87f7-48c3-a639-ec8134eb2763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-transform target variable to stabilize variance\n",
    "df_sample['totalFare'] = np.log1p(df_sample['totalFare'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43b86c52-1b77-4f5a-ab6c-d6a451b36df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by `daysToDeparture` instead of `flightDate` (preserves time ordering)\n",
    "df_sample = df_sample.sort_values(by=['daysToDeparture'], ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff1b1499-21fd-48bb-94e2-d07da552fbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure `durationToDistanceRatio` exists\n",
    "df_sample['durationToDistanceRatio'] = df_sample['totalAirtime'] / df_sample['totalTravelDistance']\n",
    "df_sample['durationToDistanceRatio'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_sample.dropna(subset=['durationToDistanceRatio'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d88f180-9c7c-45a7-ba98-e1dd9451ec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define key features\n",
    "features = ['daysToDeparture', 'pricePerMile', 'isHoliday', 'preHolidayFlight', \n",
    "            'postHolidayFlight', 'totalLayoverTime', 'durationToDistanceRatio']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99685845-0cfc-4a72-8c51-449824bf7073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Lag Features (LSTM needs sequential data)\n",
    "df_sample['fareLag_1'] = df_sample['totalFare'].shift(1)\n",
    "df_sample['fareLag_7'] = df_sample['totalFare'].shift(7)\n",
    "features += ['fareLag_1', 'fareLag_7']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb2448ec-82df-4a0d-91e5-48a83a19aa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaNs caused by shifting\n",
    "df_sample.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4ab4864-fbf0-4d26-91df-45356f662d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "X = df_sample[features]\n",
    "y = df_sample['totalFare']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83c78f47-fb95-4956-9c4d-c2becbf26f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "513ff1ca-dd12-4502-abdd-b49836e74910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays\n",
    "X_array = np.array(X_scaled)\n",
    "y_array = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e65f82c-64c2-412a-b527-c392d9d79900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **SEQUENCE CREATION (Sequence Length = 7, Batch Size = 64)**\n",
    "sequence_length = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9b5fed3-a201-48e3-8b3f-0ccb3161cea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(X, y, seq_length, batch_size=64):\n",
    "    \"\"\"Creates a tf.data.Dataset for efficient sequence batching.\"\"\"\n",
    "    def generator():\n",
    "        for i in range(len(X) - seq_length):\n",
    "            yield X[i:i+seq_length], y[i+seq_length]\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(seq_length, X.shape[1]), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(), dtype=tf.float32)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return dataset.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)  # Speed up training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9373ceb8-b701-44c4-b3d7-23612d156e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Train-Test Split (80% Train, 10% Validation, 10% Test)**\n",
    "train_size = int(len(X_array) * 0.8)\n",
    "X_train, X_test = X_array[:train_size], X_array[train_size:]\n",
    "y_train, y_test = y_array[:train_size], y_array[train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "313686ff-bc16-4715-88e8-446bd5df4f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further split test set into validation and test (50/50 split)\n",
    "val_size = int(len(X_test) * 0.5)\n",
    "X_val, X_test = X_test[:val_size], X_test[val_size:]\n",
    "y_val, y_test = y_test[:val_size], y_test[val_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "610a6a31-298c-4058-9cce-019648cefd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tf.data.Dataset\n",
    "train_dataset = create_tf_dataset(X_train, y_train, sequence_length, batch_size=64)\n",
    "val_dataset = create_tf_dataset(X_val, y_val, sequence_length, batch_size=64)\n",
    "test_dataset = create_tf_dataset(X_test, y_test, sequence_length, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f00e2e99-44e5-4edc-b0c0-d1f133104397",
   "metadata": {},
   "outputs": [],
   "source": [
    "### **GRID SEARCH (LIMITED TO 2 VALUES PER PARAMETER)**\n",
    "best_mae = float('inf')\n",
    "best_params = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e7474a2-f544-4ee8-a0f8-6ee162d5e944",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'lstm_units': [32, 50],  \n",
    "    'dropout_rate': [0.2, 0.3],  \n",
    "    'batch_size': [32, 64]  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6e507db-2f86-4828-b9c2-8c1805467c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allison Conrey\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "C:\\ProgramData\\anaconda3\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allison Conrey\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "C:\\ProgramData\\anaconda3\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allison Conrey\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "C:\\ProgramData\\anaconda3\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allison Conrey\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "C:\\ProgramData\\anaconda3\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allison Conrey\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "C:\\ProgramData\\anaconda3\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allison Conrey\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "C:\\ProgramData\\anaconda3\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allison Conrey\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "C:\\ProgramData\\anaconda3\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allison Conrey\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "C:\\ProgramData\\anaconda3\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step\n",
      "Best Hyperparameters: {'lstm_units': 32, 'dropout_rate': 0.3, 'batch_size': 64}\n"
     ]
    }
   ],
   "source": [
    "for lstm_units in param_grid['lstm_units']:\n",
    "    for dropout_rate in param_grid['dropout_rate']:\n",
    "        for batch_size in param_grid['batch_size']:\n",
    "            # Build LSTM Model\n",
    "            model = Sequential([\n",
    "                LSTM(lstm_units, return_sequences=True, input_shape=(sequence_length, X_train.shape[1])),\n",
    "                Dropout(dropout_rate),\n",
    "                BatchNormalization(),\n",
    "\n",
    "                LSTM(lstm_units, return_sequences=False),\n",
    "                Dropout(dropout_rate),\n",
    "\n",
    "                Dense(16, activation='relu'),\n",
    "                Dense(1)\n",
    "            ])\n",
    "\n",
    "            model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "\n",
    "            # Train Model with Early Stopping\n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "            history = model.fit(train_dataset, validation_data=val_dataset, epochs=10, \n",
    "                                batch_size=batch_size, callbacks=[early_stop], verbose=0)\n",
    "\n",
    "            # Evaluate on Validation Data\n",
    "            y_val_pred = np.expm1(model.predict(val_dataset).squeeze())\n",
    "            y_val_real = np.expm1(y_val[-len(y_val_pred):])\n",
    "\n",
    "            mae = mean_absolute_error(y_val_real, y_val_pred)\n",
    "\n",
    "            if mae < best_mae:\n",
    "                best_mae = mae\n",
    "                best_params = {'lstm_units': lstm_units, 'dropout_rate': dropout_rate, 'batch_size': batch_size}\n",
    "\n",
    "print(f\"Best Hyperparameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e45803ce-82d3-4349-9ed2-ead7d6e72961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allison Conrey\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "### **FINAL MODEL TRAINING WITH BEST PARAMETERS**\n",
    "final_model = Sequential([\n",
    "    LSTM(best_params['lstm_units'], return_sequences=True, input_shape=(sequence_length, X_train.shape[1])),\n",
    "    Dropout(best_params['dropout_rate']),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    LSTM(best_params['lstm_units'], return_sequences=False),\n",
    "    Dropout(best_params['dropout_rate']),\n",
    "\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "final_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60ab28cf-ef61-48b2-a80e-88d14b167894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "  24994/Unknown \u001b[1m164s\u001b[0m 6ms/step - loss: 0.6681 - mae: 0.4846"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25000/25000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 7ms/step - loss: 0.6680 - mae: 0.4846 - val_loss: 0.2037 - val_mae: 0.3449 - learning_rate: 0.0010\n",
      "Epoch 2/15\n",
      "\u001b[1m25000/25000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 7ms/step - loss: 0.1794 - mae: 0.3120 - val_loss: 0.2037 - val_mae: 0.3450 - learning_rate: 0.0010\n",
      "Epoch 3/15\n",
      "\u001b[1m25000/25000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 7ms/step - loss: 0.1790 - mae: 0.3117 - val_loss: 0.2034 - val_mae: 0.3434 - learning_rate: 0.0010\n",
      "Epoch 4/15\n",
      "\u001b[1m25000/25000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 7ms/step - loss: 0.1779 - mae: 0.3110 - val_loss: 0.2036 - val_mae: 0.3445 - learning_rate: 0.0010\n",
      "Epoch 5/15\n",
      "\u001b[1m24995/25000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1772 - mae: 0.3104\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m25000/25000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 7ms/step - loss: 0.1772 - mae: 0.3104 - val_loss: 0.2039 - val_mae: 0.3444 - learning_rate: 0.0010\n",
      "Epoch 6/15\n",
      "\u001b[1m25000/25000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 7ms/step - loss: 0.1763 - mae: 0.3097 - val_loss: 0.2046 - val_mae: 0.3446 - learning_rate: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# **Final Training (15 Epochs, Early Stop, Reduce LR)**\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)\n",
    "\n",
    "history = final_model.fit(train_dataset, validation_data=val_dataset, epochs=15, \n",
    "                          batch_size=best_params['batch_size'], callbacks=[early_stop, reduce_lr], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6857f4ea-7a1d-4440-8643-95fc2bc1d4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 19ms/step\n"
     ]
    }
   ],
   "source": [
    "### **MODEL EVALUATION ON TEST DATA**\n",
    "y_pred = np.expm1(final_model.predict(test_dataset).squeeze())\n",
    "y_test_real = np.expm1(y_test[-len(y_pred):])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4f0c9e3-761b-4d99-9301-74c086366efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test MAE: 120.3741\n",
      "Final Test R² Score: -0.0457\n"
     ]
    }
   ],
   "source": [
    "# **Compute Final Metrics**\n",
    "final_mae = mean_absolute_error(y_test_real, y_pred)\n",
    "final_r2 = r2_score(y_test_real, y_pred)\n",
    "\n",
    "print(f\"Final Test MAE: {final_mae:.4f}\")\n",
    "print(f\"Final Test R² Score: {final_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9328eb59-b494-4595-b858-477967555cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained LSTM model in the 'models' folder\n",
    "import os\n",
    "\n",
    "models_dir = \"models\"\n",
    "os.makedirs(models_dir, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "model_filename = os.path.join(models_dir, \"lstm_model.h5\")\n",
    "final_model.save(model_filename)\n",
    "print(f\"LSTM model saved as {model_filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
